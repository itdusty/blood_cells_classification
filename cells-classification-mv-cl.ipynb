{"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7136932,"sourceType":"datasetVersion","datasetId":4118400},{"sourceId":7140103,"sourceType":"datasetVersion","datasetId":4120898}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Data preparations","metadata":{"id":"nHKaApd7yxuZ"}},{"cell_type":"code","source":"from google.colab import drive\ndrive.mount('/content/drive', force_remount=True)","metadata":{"id":"WVNkRk8-zBug","outputId":"26a4779f-ad26-4ab2-8212-b5b39d597147","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install neptune lightning pytorchvideo","metadata":{"id":"G6WJsfm8yzBZ","outputId":"cbfed9d0-06ef-4aa9-d89c-52d81fe30225","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import neptune\n\nrun = neptune.init_run(\n    project=\"afonchikovd585/Cells-classification\",\n    api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiJkMmY4NmNkZC05OTJlLTQwZGQtOTAwZC1kYzU1MTUwMzRjMzYifQ==\",\n)  # your credentials","metadata":{"id":"vi3a5yNHSnQG","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pytorchvideo.data import LabeledVideoDataset, make_clip_sampler, labeled_video_dataset\nfrom pytorchvideo.transforms import (\n    ApplyTransformToKey,\n    Normalize,\n    RandomShortSideScale,\n    UniformTemporalSubsample,\n    Permute\n)\nfrom torchvision.transforms import (\n    Compose,\n    Lambda,\n    RandomCrop,\n    RandomHorizontalFlip,\n    RandomResizedCrop,\n    Resize\n)\nfrom torchvision.transforms._transforms_video import (\n    CenterCropVideo,\n    NormalizeVideo\n)\n\nimport pandas as pd\nimport numpy as np\nimport os\nimport shutil\nfrom torch.utils.data import DataLoader\nfrom pytorch_lightning.callbacks import ModelCheckpoint, LearningRateMonitor\n\nimport torch\nimport torchvision\nimport torch.nn as nn\nfrom pytorch_lightning import LightningModule, seed_everything, Trainer\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nfrom sklearn.metrics import classification_report\nimport torchmetrics\n\n\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, f1_score, precision_score, recall_score, accuracy_score, roc_auc_score\nfrom math import ceil\nimport time\n#Plotting\nimport matplotlib.pyplot as plt","metadata":{"id":"04mrmwwoyxub","outputId":"020e538c-4dea-4842-8ca7-86c688b8fb77","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Multi-view mode (the use of 'num_views' augmented copies of a sample for predictions)\nmultiView = {'isMultiView':True, 'num_views': 7}\n\n#Random resized crop scale\nRRCropScale = (1E-1, 2E-1)#(1E-1, 3E-1) # without multi-view\nRRCropScaleMV = (1E-1, 2E-1)# with multi-view\n\n# Ensure that all operations are deterministic on GPU (if used) for reproducibility\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n#Visualization\nfigSize = (3,3)\nnSamples = 16\n\nclass_names = ['0', '1']\nmean = np.array([0.9133, 0.2737, 0.2737])\nstd  = np.array([0.1576, 0.2508, 0.2508])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"video_transforms = Compose([\n    ApplyTransformToKey(key='video',\n    transform=Compose([\n        #UniformTemporalSubsample(20),\n        RandomResizedCrop(64,scale=RRCropScaleMV,antialias=True),\n        RandomHorizontalFlip(p=0.5),\n        Lambda(lambda x: x / 255.0),\n        Normalize(mean,std),\n        # RandomShortSideScale(min_size=248, max_size=256),\n        # CenterCropVideo(224),\n    ]),\n    ),\n])","metadata":{"id":"TvTfp6DMyxud","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def accuracy_mv(labels,pred):\n  with torch.no_grad():\n\n    n_samples = labels.shape[0]\n    n_correct = (pred == labels).sum().item()\n\n    acc_without_u = (n_correct / n_samples)\n\n    accuracy_mv_attr = {\n        'pred':pred,\n        'n_samples':n_samples,\n        'n_correct':n_correct,\n        'acc_without_u':acc_without_u,\n        }\n    return accuracy_mv_attr","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def mvc(mode_template,predictions_mv,values_mv,labels):\n    ''' Inputs:\n        mode_template is a zero-like tensor\n        predictions_mv are multi view predictions of classes, (0,..., NUM_CLASSES)\n        values_mv are the maximum values of probabilities or certainties (0,...,1)\n        labels are labels\n\n        Outputs:\n        predictions_w are weighted predictions\n    '''\n    l = mode_template.size()\n    predictions_w = mode_template * 0 #times zero just in case\n    for i in range(l[0]):\n        mvp = predictions_mv[:,i]\n        mvw = values_mv[:,i]#torch.softmax(values_mv[:,i],-1)# / values_mv[:,i].sum()\n\n        freq_w = torch.bincount(mvp, weights=mvw)\n        _, predictions_w[i] = torch.max(freq_w,0)\n        if i < nSamples: print(mvp, mvw, predictions_w[i].item(), labels[i].item(),\n                                class_names[predictions_w[i].item()], class_names[labels[i].item()])\n    return predictions_w\n\n\ndef multiViewWeightedPred(labels_mv,predictions_mv,probabilities_mv):\n    mode_labels,_ = torch.mode(labels_mv,dim=0)\n    mode_template = torch.zeros_like(mode_labels)\n\n    print('probabilities')\n    predictionsPB = mvc(mode_template,predictions_mv,probabilities_mv,mode_labels)\n\n    weightedPred = {\n        'predictionsPB':predictionsPB,\n    }\n\n    return weightedPred","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ns = {'train': 0.6, 'val': 0.2,  'test': 0.2}\n# Function for setting the seed\nseed = 42 # random seeds are 42, 0, 17, 9, 3\ntarget_label = 'high_rbc'\nnum_classes = 2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"run['seed'] = seed","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def test_visualization(model, dataloader, nc=num_classes):\n    test_pred = []\n    test_prob = []\n    test_prob_comp = []\n    test_labe = []\n    test_X = []\n    with torch.no_grad():\n        for group in dataloader:\n            inputs = group['video'].to(DEVICE)\n            labels = group[model.target_label].to(DEVICE)\n            x = model(inputs).to(DEVICE)\n\n            prob = nn.functional.softmax(x[:,:nc],1)\n\n            probv, pred = torch.max(prob, 1)\n\n            test_pred.append(pred)\n            test_prob.append(probv)\n            test_prob_comp.append(prob)\n            test_labe.append(labels)\n            test_X.append(inputs)\n\n    test_labels = torch.cat(test_labe,dim=0)\n    test_predictions = torch.cat(test_pred,dim=0)\n    test_probabilities=torch.cat(test_prob,dim=0)\n    test_prob_complete=torch.cat(test_prob_comp,dim=0)\n    test_coord = torch.cat(test_X,dim=0)\n\n    test_inputs = test_coord\n\n    test_attr = {\n        'test_labels':test_labels,\n        'test_predictions':test_predictions,\n        'test_probabilities':test_probabilities,\n        'test_inputs':test_inputs,\n    }\n\n    return test_attr","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset_path = \"/kaggle/input/cells-classification/dataset\"","metadata":{"id":"qubRA-qDdC3r","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.chdir(dataset_path)\nos.chdir(\"..\")","metadata":{"id":"t7TgA0_oAgGu","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seed_everything(seed)\ndataframe = pd.read_csv(\"DataFrame.csv\", index_col=0)\ndataframe = dataframe.sample(frac=1, random_state=seed)\ntrain_size = int(dataframe.shape[0] * ns['train'])\ntrain_data = dataframe[0:train_size]\ntest_data = dataframe[train_size:]\ntest_size = int(dataframe.shape[0] * ns['test'])\nval_data = test_data[0:test_size]\ntest_data = test_data[test_size:]\ntrain_data, val_data, test_data","metadata":{"id":"qD43cH8J_tX4","outputId":"daad7246-c4f5-4ad2-8b1e-3e3f05fbecf3","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CustomDataset(LabeledVideoDataset):\n    def __init__(self, dataset_path, dataframe, target_name, transforms, clip_sampler_type='random', clip_duration=1):\n      df = dataframe.reset_index()\n      paths = []\n      for i, file_name in enumerate(df['files']):\n          temp_dict = df.iloc[i].to_dict()\n          temp_dict['label'] = df[target_name][i]\n          temp_dict.pop('files')\n          temp_dict.pop('index')\n          paths.append((f\"{dataset_path}/{file_name}\", temp_dict))\n      super().__init__(labeled_video_paths=paths,\n                       clip_sampler=make_clip_sampler(clip_sampler_type, clip_duration),\n                       transform=transforms, decode_audio=False)","metadata":{"id":"bm40twbHybAo","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Used for testing theory, ignore that\ntrain_dataset = CustomDataset(dataset_path=dataset_path, dataframe=train_data,\n                              target_name='high_rbc',\n                              transforms=video_transforms)\ntrain_dataset.num_videos","metadata":{"id":"IevznUsNmnk2","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"checkpoint_callback = ModelCheckpoint(save_weights_only=True, \n                                      mode=\"min\", \n                                      monitor=\"val/loss\",\n                                      dirpath=\"checkpoints\", \n                                      filename=\"file\")\nlr_monitor = LearningRateMonitor(logging_interval=\"epoch\")","metadata":{"id":"Z8boZL_OrZSG","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CurriculumTrainer():\n    def set_difficulty(self, dataframe, target_cells, target_name, alpha, beta, gamma):\n        # Calculating distances between mean and target value\n        df = dataframe.copy(deep=True)\n        df['distances'] = (df['rbc'] - df['rbc'].mean()).abs()\n\n        # Normalizing blur and distances\n        df['blur'] = (df['blur'] - df['blur'].min()) / (df['blur'].max() - df['blur'].min())\n        df['distances'] = (df['distances'] - df['distances'].min()) / (df['distances'].max() - df['distances'].min())\n\n        # Calculating and normalizing difficulty\n        df['difficulty'] = alpha * df['blur'] + beta * df['noise'] + gamma * df['distances']\n        df['difficulty'] = (df['difficulty'] - df['difficulty'].min()) / (df['difficulty'].max() - df['difficulty'].min())\n        return df\n\n    def evaluate_competence(self, max_epochs, current_epoch, c0, p):\n        return min(1, (current_epoch*((1-c0**p)/max_epochs)+c0**p)**(1/p))\n\n    def fit(self, model, dataframe, target_cells, target_name, max_epochs, c0, p, alpha, beta, gamma):\n        dataframe = self.set_difficulty(dataframe, target_cells, target_name, alpha, beta, gamma)\n        self.competence = c0\n        seed_everything(seed)\n        for epoch in range(1, max_epochs+1):\n            selected_data = dataframe[dataframe.difficulty <= self.competence]\n            dataset = CustomDataset(dataset_path=dataset_path, dataframe=selected_data,\n                                    target_name=target_name,\n                                    transforms=video_transforms)\n            loader = DataLoader(dataset, batch_size=model.batch_size, num_workers=model.numworkers, pin_memory=True)\n            print(f\"-----------------------\\nEpoch {epoch}, competence = {self.competence}, dataset size = {dataset.num_videos}\")\n            run[\"model/competence\"].append(self.competence)\n            run[\"train/dataset_size\"].append(dataset.num_videos)\n            self.trainer = Trainer(max_epochs=1,\n                                   precision='16-mixed',\n                                   accumulate_grad_batches=2,\n                                   enable_progress_bar=True,\n                                   enable_model_summary=False,\n                                   num_sanity_val_steps=0,\n                                   callbacks=[lr_monitor, checkpoint_callback])\n            self.trainer.fit(model, loader)\n            self.competence = self.evaluate_competence(max_epochs, epoch, c0, p)\n\n    def validate(self, model):\n#         dataset = CustomDataset(dataset_path=dataset_path, dataframe=test_data,\n#                                 target_name=model.target_label,\n#                                 transforms=video_transforms)\n#         loader = DataLoader(dataset, batch_size=model.batch_size, num_workers=model.numworkers, pin_memory=True)\n#         self.trainer.validate(model, loader)\n        self.trainer.validate(model)\n\n    def test(self, model):\n#         dataset = CustomDataset(dataset_path=dataset_path, dataframe=test_data,\n#                                 target_name=model.target_label,\n#                                 transforms=video_transforms)\n#         loader = DataLoader(dataset, batch_size=model.batch_size, num_workers=model.numworkers, pin_memory=True)\n#         self.trainer.test(model, loader)\n        self.trainer.test(model)","metadata":{"id":"Fa66LLF-XWKh","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model","metadata":{"id":"SgbT8X8Tyxui"}},{"cell_type":"code","source":"class TestModel(LightningModule):\n    def __init__(self):\n        super(TestModel, self).__init__()\n        # model architecture\n        self.video_model = torch.hub.load(\"facebookresearch/pytorchvideo\", \"efficient_x3d_xs\", pretrained=True)\n        self.relu = nn.ReLU()\n        self.linear = nn.Linear(400, num_classes)\n\n        self.lr = 1e-3\n        self.batch_size = 8\n        self.numworkers = 0\n        # evaluation metric\n        self.metric = torchmetrics.Accuracy(task='multiclass', num_classes=num_classes)\n        # loss function\n        self.criterion = nn.CrossEntropyLoss()\n        # helpers\n        self.target_label = target_label\n        self.training_step_outputs = []\n        self.validation_step_outputs = []\n        self.testing_step_outputs = []\n\n    def forward(self, x):\n        x = self.video_model(x)\n        x = self.relu(x)\n        x = self.linear(x)\n        return x\n\n    def configure_optimizers(self):\n        opt = torch.optim.AdamW(params=self.parameters(), lr=self.lr)\n        scheduler = CosineAnnealingLR(opt, T_max=10, eta_min=1e-6, last_epoch=-1)\n        return {'optimizer': opt, 'lr_scheduler': scheduler}\n    \n    # This should remain commented out because in CurriculumTrainer dataloader created in every epoch.\n    # Do not uncomment\n    # def train_dataloader(self):\n    #     dataset = CustomDataset(dataset_path=dataset_path, dataframe=train_data,\n    #                           target_name=self.target_label,\n    #                           transforms=video_transforms)\n    #     loader = DataLoader(dataset, batch_size=self.batch_size, num_workers=self.numworkers, pin_memory=True)\n    #     return loader\n\n    def training_step(self, batch, batch_idx):\n        video, label = batch['video'], batch['label']\n        out = self.forward(video)\n        loss = self.criterion(out, label)\n        metric = self.metric(out, label.to(torch.int64))\n        self.training_step_outputs.append({'loss': loss, 'metric': metric})\n        return {'loss': loss, 'metric': metric}\n\n    def on_train_epoch_end(self):\n        outputs = self.training_step_outputs\n        loss = torch.stack([x['loss'] for x in outputs]).mean().cpu().detach().numpy().round(2)\n        metric = torch.stack([x['metric'] for x in outputs]).mean().cpu().detach().numpy().round(2)\n        self.training_step_outputs = []\n        self.log('train/loss', loss)\n        self.log('train/metric', metric)\n        run[\"train/loss\"].append(loss)\n        run[\"train/metric\"].append(metric)\n\n    def val_dataloader(self):\n        dataset = CustomDataset(dataset_path=dataset_path, dataframe=val_data,\n                              target_name=self.target_label,\n                              transforms=video_transforms)\n        loader = DataLoader(dataset, batch_size=self.batch_size, num_workers=self.numworkers, pin_memory=True)\n        return loader\n\n    def validation_step(self, batch, batch_idx):\n        video, label = batch['video'], batch['label']\n        out = self.forward(video)\n        loss = self.criterion(out, label)\n        metric = self.metric(out, label.to(torch.int64))\n        self.validation_step_outputs.append({'loss': loss, 'metric': metric})\n        return {'loss': loss, 'metric': metric}\n\n    def on_validation_epoch_end(self):\n        outputs = self.validation_step_outputs\n        loss = torch.stack([x['loss'] for x in outputs]).mean().cpu().detach().numpy().round(2)\n        metric = torch.stack([x['metric'] for x in outputs]).mean().cpu().detach().numpy().round(2)\n        self.validation_step_outputs = []\n        self.log('val/loss', loss)\n        self.log('val/metric', metric)\n        run[\"val/loss\"].append(loss)\n        run[\"val/metric\"].append(metric)\n        print({'loss': loss, 'metric': metric})\n\n    def test_dataloader(self):\n        dataset = CustomDataset(dataset_path=dataset_path, dataframe=test_data,\n                              target_name=self.target_label,\n                              transforms=video_transforms)\n        loader = DataLoader(dataset, batch_size=self.batch_size, num_workers=self.numworkers, pin_memory=True)\n        return loader\n\n    def test_step(self, batch, batch_idx):\n        video, label = batch['video'], batch['label']\n        out = self.forward(video)\n        self.testing_step_outputs.append({'label': label, 'pred': out})\n        return {'label': label, 'pred': out}\n\n    def on_test_epoch_end(self):\n        outputs = self.testing_step_outputs\n        label = torch.cat([x['label'] for x in outputs]).cpu().detach().numpy()\n        pred = torch.cat([x['pred'].argmax(dim=1) for x in outputs]).cpu().detach().numpy()\n        self.testing_step_outputs = []\n        print(classification_report(label, pred))","metadata":{"id":"XEILRhilyxuj","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = TestModel()","metadata":{"id":"BDS3zCEpyxuj","outputId":"a1571cb6-1004-460d-a3d1-6e09b39e7b35","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params = {\n    \"n_epochs\": 400,\n    \"c0\": 0.05,\n    \"p\": 2,\n    # In difficulty function: alpha * df['blur'] + beta * df['noise'] + gamma * df['distances']\n    \"alpha\": 0.5, \n    \"beta\": 0, # Do not use, used for testing theory\n    \"gamma\": 0.5 \n}\nrun[\"model/parameters\"] = params\nrun[\"model/architecture\"] = \"efficient_x3d_xs\"\nrun[\"model/difficulty_func\"] = \"alpha * df['blur'] + beta * df['noise'] + gamma * df['distances']\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.chdir(\"/kaggle/working/\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Run this for trainer without curriculum\nseed_everything(seed)\ntrainer = Trainer(max_epochs=params[\"n_epochs\"],\n                  precision='16-mixed',\n                  accumulate_grad_batches=2,\n                  enable_progress_bar=True,\n                  num_sanity_val_steps=0,\n                  callbacks=[lr_monitor, checkpoint_callback])\n\ndataset = CustomDataset(dataset_path=dataset_path, dataframe=train_data,\n                      target_name=model.target_label,\n                      transforms=video_transforms)\nloader = DataLoader(dataset, batch_size=model.batch_size, num_workers=model.numworkers, pin_memory=True)\ntrainer.fit(model, loader)","metadata":{"id":"X8lHfORtSnQL","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Used for testing theory, ignore that\nseed_everything(seed)\nfor epoch in range(1, params[\"n_epochs\"]+1):\n    dataset = CustomDataset(dataset_path=dataset_path, dataframe=train_data,\n                          target_name=model.target_label,\n                          transforms=video_transforms)\n    loader = DataLoader(dataset, batch_size=model.batch_size, num_workers=model.numworkers, pin_memory=True)\n    trainer = Trainer(max_epochs=1,\n                           precision='16-mixed',\n                           accumulate_grad_batches=2,\n                           enable_progress_bar=True,\n                           enable_model_summary=False,\n                           num_sanity_val_steps=0,\n                           callbacks=[lr_monitor, checkpoint_callback])\n    trainer.fit(model, loader)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Run this for curriculum learning\ntrainer = CurriculumTrainer()\nstart = time.time()\ntrainer.fit(model, train_data, 'rbc', 'high_rbc',\n            params[\"n_epochs\"], params[\"c0\"], params[\"p\"],\n            params[\"alpha\"], params[\"beta\"], params[\"gamma\"])\nstop = time.time()","metadata":{"id":"tnbeTSI8SnQM","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Elapsed time: {stop - start}\")\nrun['elapsed_time'] = stop - start","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = TestModel.load_from_checkpoint(checkpoint_callback.best_model_path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.validate(model)","metadata":{"id":"21U0bYumlkH_","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.test(model)","metadata":{"id":"aR99tqb2lyR1","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"run.stop()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = CustomDataset(dataset_path=dataset_path, dataframe=test_data,\n                              target_name=model.target_label,\n                              transforms=video_transforms)\ntest_dataloader = DataLoader(dataset, batch_size=model.batch_size, num_workers=model.numworkers, pin_memory=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_labels_mv = []\ntest_predictions_mv = []\ntest_probabilities_mv = []\n\nfor i in range(multiView['num_views']):\n    test_attr = test_visualization(model.to(DEVICE), test_dataloader)\n#     fig = plt.figure(figsize=(10, 10))\n#     out = torchvision.utils.make_grid(test_attr['test_inputs'][:nSamples].to('cpu'))\n#     imshow(out,title='Multi-view test')\n    test_labels_mv.append(test_attr['test_labels'].to('cpu').numpy())\n    test_predictions_mv.append(test_attr['test_predictions'].to('cpu').numpy())\n    test_probabilities_mv.append(test_attr['test_probabilities'].to('cpu').numpy())\n    \n    del test_attr\n\nlabels_mv = torch.as_tensor(test_labels_mv)\npredictions_mv = torch.as_tensor(test_predictions_mv)\nprobabilities_mv=torch.as_tensor(test_probabilities_mv)\n\nprint('labels',labels_mv[:,:nSamples])\nprint('predictions',predictions_mv[:,:nSamples])\nprint('probabilities', probabilities_mv[:,:nSamples])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mode_labels,_ = torch.mode(labels_mv,dim=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"weightedPred = multiViewWeightedPred(labels_mv,predictions_mv,probabilities_mv)\nprint(weightedPred)\nn_correct_MVS_PB = (weightedPred['predictionsPB'] == mode_labels).sum().item()\n\nn_samples = mode_labels.shape[0]\naccuracy_MVS_PB = n_correct_MVS_PB / n_samples\n\nprint('Soft multi view')\nprint('n_samples',n_samples)\nprint(f'accuracy of prediction based soft MV = {accuracy_MVS_PB}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cm = confusion_matrix(mode_labels.to('cpu'), weightedPred['predictionsPB'].to('cpu'))\nprint(f'Accuracy of prediction based soft MV is {accuracy_MVS_PB}')\ncm_display = ConfusionMatrixDisplay(cm,display_labels=class_names).plot()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}